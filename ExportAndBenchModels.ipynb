{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411b2b6f-dbec-4954-a7cd-6779755c2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import PIL.Image\n",
    "from tha3.util import resize_PIL_image,extract_PIL_image_from_filelike, extract_pytorch_image_from_PIL_image\n",
    "from tqdm import tqdm\n",
    "import onnx\n",
    "from onnxsim import simplify\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from typing import List, Optional\n",
    "from torch.nn.functional import interpolate\n",
    "\n",
    "MODEL_NAME = \"separable_float\"\n",
    "HALF = False\n",
    "DEVICE_NAME = 'cuda:0'\n",
    "IMAGE_INPUT = \"data\\images\\crypko_03.png\"\n",
    "USE_RANDOM_IMAGE = False\n",
    "\n",
    "providers = [(\"CUDAExecutionProvider\", {\"device_id\": 1, #torch.cuda.current_device(),\n",
    "                                        \"user_compute_stream\": str(torch.cuda.current_stream().cuda_stream)})]\n",
    "sess_options = ort.SessionOptions()\n",
    "\n",
    "device = torch.device(DEVICE_NAME)\n",
    "dtype = torch.float16 if HALF else torch.float32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b978744a-13c3-44bc-80aa-784c98a5e97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the separable_float model.\n",
      "Loading the eyebrow decomposer ... DONE!!!\n",
      "Loading the eyebrow morphing conbiner ... DONE!!!\n",
      "Loading the face morpher ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\talking-head-anime-3-demo\\tha3\\util.py:247: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!!!\n",
      "Loading the face-body rotator ... DONE!!!\n",
      "Loading the combiner ... DONE!!!\n"
     ]
    }
   ],
   "source": [
    "#Prepare models\n",
    "def load_poser(model: str, device: torch.device):\n",
    "    print(\"Using the %s model.\" % model)\n",
    "    if model == \"standard_float\":\n",
    "        from tha3.poser.modes.standard_float import create_poser\n",
    "        return create_poser(device)\n",
    "    elif model == \"standard_half\":\n",
    "        from tha3.poser.modes.standard_half import create_poser\n",
    "        return create_poser(device)\n",
    "    elif model == \"separable_float\":\n",
    "        from tha3.poser.modes.separable_float import create_poser\n",
    "        return create_poser(device)\n",
    "    elif model == \"separable_half\":\n",
    "        from tha3.poser.modes.separable_half import create_poser\n",
    "        return create_poser(device)\n",
    "    else:\n",
    "        raise RuntimeError(\"Invalid model: '%s'\" % model)\n",
    "        \n",
    "poser = load_poser(MODEL_NAME, DEVICE_NAME)\n",
    "pose_size = poser.get_num_parameters()\n",
    "\n",
    "eyebrow_decomposer = poser.get_modules()['eyebrow_decomposer']\n",
    "eyebrow_morphing_combiner = poser.get_modules()['eyebrow_morphing_combiner']\n",
    "face_morpher = poser.get_modules()['face_morpher']\n",
    "two_algo_face_body_rotator = poser.get_modules()['two_algo_face_body_rotator']\n",
    "editor = poser.get_modules()['editor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a1a1c07-6aaf-477a-a068-4eff34aa4497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare one pass inference image data\n",
    "pt_img = None\n",
    "if USE_RANDOM_IMAGE:\n",
    "    pt_img = torch.rand(1, 4, 512, 512,dtype=dtype, device=device) * 2.0 - 1.0\n",
    "else:\n",
    "    pil_image = resize_PIL_image(extract_PIL_image_from_filelike(IMAGE_INPUT), size=(512,512))\n",
    "    \n",
    "    if HALF:\n",
    "        pt_img = extract_pytorch_image_from_PIL_image(pil_image).half().reshape(1,4,512,512).to(DEVICE_NAME)\n",
    "    else:\n",
    "        pt_img = extract_pytorch_image_from_PIL_image(pil_image).reshape(1,4,512,512).to(DEVICE_NAME)\n",
    "zero_pose = torch.zeros(1, pose_size, dtype=dtype, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5099c456-c77d-4cdf-86ef-96d163528def",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 24.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.101931571960449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Small bench for whole pose\n",
    "from time import time\n",
    "t1 = time()\n",
    "for i in tqdm(range(100)):\n",
    "    poser.pose(pt_img, zero_pose)\n",
    "print(time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d879bd6-a270-4113-82ef-9594e695508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on eyebrow decomposer export and quantize\n",
    "EYEBROW_DECOMPOSER_ONNX_MODEL_NAME = \"eyebrow_decomposer.onnx\"\n",
    "EYEBROW_DECOMPOSER_SIMPLIFIED_ONNX_MODEL_NAME = \"eyebrow_decomposer_sim.onnx\"\n",
    "EYEBROW_DECOMPOSER_INPUT_LIST = ['input_image']\n",
    "EYEBROW_DECOMPOSER_OUTPUT_LIST = [\"eyebrow_layer\", \"background_layer_1\"]\n",
    "EYEBROW_DECOMPOSER_INPUT_SHAPE = (1,4,128,128)\n",
    "EYEBROW_DECOMPOSER_INPUT_IMAGE_SIZE = 128\n",
    "\n",
    "eyebrow_decomposer_input_img = pt_img[:,:, 64:192, 64 + 128:192 + 128].reshape(EYEBROW_DECOMPOSER_INPUT_SHAPE)\n",
    "eyebrow_decomposer_input_img_numpy = eyebrow_decomposer_input_img.cpu().numpy()\n",
    "eyebrow_decomposer_torch_res = eyebrow_decomposer(eyebrow_decomposer_input_img) # Try one round inference to catch problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90d00b91-8bc1-4573-bf99-c2d65774a1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyebrowDecomposerWrapper(Module):\n",
    "    def __init__(self, eyebrow_decomposer_obj):\n",
    "        super().__init__()\n",
    "        self.eyebrow_decomposer = eyebrow_decomposer_obj\n",
    "    def forward(self, image: Tensor, *args) -> List[Tensor]:\n",
    "        cropped = image[:,:, 64:192, 64 + 128:192 + 128].reshape((1,4,128,128))\n",
    "        decomposer_res = self.eyebrow_decomposer(cropped)\n",
    "        return [decomposer_res[0], decomposer_res[3]]\n",
    "eyebrow_decomposer_wrapper = EyebrowDecomposerWrapper(eyebrow_decomposer)\n",
    "eyebrow_decomposer_wrapped_torch_res = eyebrow_decomposer_wrapper(pt_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06b39574-329a-4436-a922-50cebfeeeb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\envs\\talking-head-anime-3-dev\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:1531: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'instance_norm' is set to train=True. Exporting with train=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Export onnx model finally get a simplified decomposer onnx model\n",
    "torch.onnx.export(eyebrow_decomposer_wrapper,               # model being run\n",
    "                  pt_img,                         # model input (or a tuple for multiple inputs)\n",
    "                  EYEBROW_DECOMPOSER_ONNX_MODEL_NAME,   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=16,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = EYEBROW_DECOMPOSER_INPUT_LIST,   # the model's input names\n",
    "                  output_names = EYEBROW_DECOMPOSER_OUTPUT_LIST) # the model's output names\n",
    "onnx_model = onnx.load(EYEBROW_DECOMPOSER_ONNX_MODEL_NAME)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "onnx_model_sim, check = simplify(onnx_model)\n",
    "if check:\n",
    "    onnx.save(onnx_model_sim, EYEBROW_DECOMPOSER_SIMPLIFIED_ONNX_MODEL_NAME)\n",
    "else:\n",
    "    print(\"Simplify error!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd7a2df1-5737-46bc-91e8-95c675b9ee9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is:  1.4267975e-09\n",
      "MSE is:  1.0202475e-10\n"
     ]
    }
   ],
   "source": [
    "input_dict = {k:v.cpu().detach().numpy() for k,v in zip(EYEBROW_DECOMPOSER_INPUT_LIST, (pt_img))}\n",
    "# Verify correctness compare to pytorch\n",
    "pt_img_np = pt_img.cpu().detach().numpy()\n",
    "ort_sess = ort.InferenceSession(EYEBROW_DECOMPOSER_SIMPLIFIED_ONNX_MODEL_NAME, sess_options=sess_options, providers=providers)\n",
    "onnx_sim_output = ort_sess.run(None, {'input_image':pt_img_np,})\n",
    "print(\"MSE is: \",((onnx_sim_output[0] - eyebrow_decomposer_torch_res[0].cpu().detach().numpy()) ** 2).mean())\n",
    "print(\"MSE is: \",((onnx_sim_output[1] - eyebrow_decomposer_torch_res[3].cpu().detach().numpy()) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0b2c294-ee7c-4f67-b140-119e6158cbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 46.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.147073268890381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Small bench on cpu to check performance\n",
    "t1 = time()\n",
    "for i in tqdm(range(100)):\n",
    "    ort_sess.run(None, {'input_image':pt_img_np,})\n",
    "print(time()-t1)\n",
    "#Since decomposer is not required for every run iteration, no need to quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19309a93-5219-4d58-ad15-a5eb1277f8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\talking-head-anime-3-demo\\tha3\\nn\\image_processing_util.py:64: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if n == self.last_n and device == self.last_device:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is:  1.060896e-11\n"
     ]
    }
   ],
   "source": [
    "# Start working on eyebrow morphine combiner\n",
    "\n",
    "EYEBROW_MORPHING_COMBINER_INPUT_LIST = ['input_image','eyebrow_background_layer', \"eyebrow_layer\", 'eyebrow_pose']\n",
    "EYEBROW_MORPHING_COMBINER_OUTPUT_LIST = ['eyebrow_image']  # 7\n",
    "EYEBROW_POSE_SHAPE = (1, 12)\n",
    "EYEBROW_MORPHING_COMBINER_ONNX_MODEL_NAME = \"eyebrow_morphing_combiner.onnx\"\n",
    "EYEBROW_MORPHING_COMBINER_SIMPLIFIED_ONNX_MODEL_NAME = \"eyebrow_morphing_combiner_sim.onnx\"\n",
    "\n",
    "\n",
    "eyebrow_pose_zero = torch.zeros(EYEBROW_POSE_SHAPE, dtype=dtype, device=device)\n",
    "eyebrow_morphing_combiner_torch_res = eyebrow_morphing_combiner(eyebrow_decomposer_wrapped_torch_res[1], \n",
    "                                                                eyebrow_decomposer_wrapped_torch_res[0], eyebrow_pose_zero)\n",
    "\n",
    "#Build a new eyebrow_morphing_combiner that does cropping\n",
    "class EyebrowMorphingCombinerWrapper(Module):\n",
    "    def __init__(self, eyebrow_morphing_combiner_obj):\n",
    "        super().__init__()\n",
    "        self.eyebrow_morphing_combiner = eyebrow_morphing_combiner_obj\n",
    "    def forward(self, full_image:Tensor, background_layer: Tensor, eyebrow_layer: Tensor, pose: Tensor, *args) -> Tensor:\n",
    "        im_morpher_crop = full_image[:, :, 32:32 + 192, (32 + 128):(32 + 192 + 128)].clone()\n",
    "        im_morpher_crop[:, :, 32:32 + 128, 32:32 + 128] = self.eyebrow_morphing_combiner(background_layer, eyebrow_layer, pose)[2]\n",
    "        return im_morpher_crop\n",
    "eyebrow_morphing_combiner_wrapped = EyebrowMorphingCombinerWrapper(eyebrow_morphing_combiner)\n",
    "eyebrow_morphing_combiner_wrapped_torch_res = eyebrow_morphing_combiner_wrapped(pt_img, eyebrow_decomposer_wrapped_torch_res[1], \n",
    "                                                                                eyebrow_decomposer_wrapped_torch_res[0], eyebrow_pose_zero)\n",
    "\n",
    "\n",
    "input_tuple = (pt_img, eyebrow_decomposer_wrapped_torch_res[1], eyebrow_decomposer_wrapped_torch_res[0], eyebrow_pose_zero)\n",
    "input_dict = {k:v.cpu().detach().numpy() for k,v in zip(EYEBROW_MORPHING_COMBINER_INPUT_LIST,input_tuple)}\n",
    "torch.onnx.export(eyebrow_morphing_combiner_wrapped,               # model being run\n",
    "                  input_tuple,                         # model input (or a tuple for multiple inputs)\n",
    "                  EYEBROW_MORPHING_COMBINER_ONNX_MODEL_NAME,   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=16,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = EYEBROW_MORPHING_COMBINER_INPUT_LIST,   # the model's input names\n",
    "                  output_names = EYEBROW_MORPHING_COMBINER_OUTPUT_LIST) \n",
    "onnx_model = onnx.load(EYEBROW_MORPHING_COMBINER_ONNX_MODEL_NAME)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "eyebrow_morphing_combiner_onnx_model_sim, check = simplify(onnx_model)\n",
    "if check:\n",
    "    onnx.save(eyebrow_morphing_combiner_onnx_model_sim,EYEBROW_MORPHING_COMBINER_SIMPLIFIED_ONNX_MODEL_NAME)\n",
    "else:\n",
    "    print(\"Simplify error!\")\n",
    "\n",
    "# Verify correctness compare to pytorch\n",
    "ort_sess_sim = ort.InferenceSession(EYEBROW_MORPHING_COMBINER_SIMPLIFIED_ONNX_MODEL_NAME, sess_options=sess_options, providers=providers)\n",
    "onnx_sim_output = ort_sess_sim.run(EYEBROW_MORPHING_COMBINER_OUTPUT_LIST,input_dict)\n",
    "for i in range(len(onnx_sim_output)):\n",
    "    print(\"MSE is: \",((onnx_sim_output[i] - eyebrow_morphing_combiner_wrapped_torch_res[i].cpu().detach().numpy()) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7dd96ac-0d84-4c3a-8b1f-3c1279b5a798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:02<00:00, 43.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.315666913986206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Small bench on cpu to check performance\n",
    "t1 = time()\n",
    "for i in tqdm(range(100)):\n",
    "    ort_sess_sim.run(EYEBROW_MORPHING_COMBINER_OUTPUT_LIST,input_dict)\n",
    "print(time()-t1)\n",
    "#Since eyebrow morpher is not required for every run iteration, no need to quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bde98d0-2c20-49f3-a3fc-d84d92072f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is:  2.885212e-10\n"
     ]
    }
   ],
   "source": [
    "#Play with face morpher\n",
    "FACE_POSE_SHAPE = (1,27)\n",
    "face_pose_zero = torch.zeros(FACE_POSE_SHAPE, dtype=dtype, device=device)\n",
    "face_morpher_torch_res = face_morpher(eyebrow_morphing_combiner_wrapped_torch_res, face_pose_zero)\n",
    "\n",
    "class FaceMorpherWrapped(Module):\n",
    "    def __init__(self, face_morpher_obj):\n",
    "        super().__init__()\n",
    "        self.face_morpher = face_morpher_obj\n",
    "    def forward(self, input_image: Tensor, im_morpher_crop: Tensor, face_pose:Tensor,  *args) -> List[Tensor]:\n",
    "        face_morphed_full = input_image.clone()\n",
    "        face_morphed_full[:, :, 32:32 + 192, 32 + 128:32 + 192 + 128] = self.face_morpher(im_morpher_crop, face_pose)[0]\n",
    "        face_morphed_half = interpolate(face_morphed_full, size=(256, 256), mode='bilinear', align_corners=False)\n",
    "        return [face_morphed_full, face_morphed_half]\n",
    "face_morpher_wrapped = FaceMorpherWrapped(face_morpher)\n",
    "face_morpher_wrapped_torch_res = face_morpher_wrapped(pt_img, eyebrow_morphing_combiner_wrapped_torch_res, face_pose_zero) #May be error\n",
    "\n",
    "FACE_MORPHER_ONNX_MODEL_NAME = 'face_morpher.onnx'\n",
    "FACE_MORPHER_SIM_ONNX_MODEL_NAME = 'face_morpher_sim.onnx'\n",
    "\n",
    "FACE_MORPHER_OUTPUT_LIST = ['face_morphed_full', 'face_morphed_half']\n",
    "FACE_MORPHER_INPUT_LIST = ['image_input', 'im_morpher_crop', 'face_pose']\n",
    "input_tuple = (pt_img, eyebrow_morphing_combiner_wrapped_torch_res, face_pose_zero)\n",
    "input_dict = {k:v.cpu().detach().numpy() for k,v in zip(FACE_MORPHER_INPUT_LIST,input_tuple)}\n",
    "\n",
    "torch.onnx.export(face_morpher_wrapped,               # model being run\n",
    "                  input_tuple,                         # model input (or a tuple for multiple inputs)\n",
    "                  FACE_MORPHER_ONNX_MODEL_NAME,   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=16,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = FACE_MORPHER_INPUT_LIST,   # the model's input names\n",
    "                  output_names = FACE_MORPHER_OUTPUT_LIST) \n",
    "\n",
    "onnx_model = onnx.load(FACE_MORPHER_ONNX_MODEL_NAME)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "onnx_model_sim, check = simplify(onnx_model)\n",
    "if check:\n",
    "    onnx.save(onnx_model_sim,FACE_MORPHER_SIM_ONNX_MODEL_NAME)\n",
    "else:\n",
    "    print(\"Simplify error!\")\n",
    "\n",
    "ort_sess_sim = ort.InferenceSession(FACE_MORPHER_SIM_ONNX_MODEL_NAME, sess_options=sess_options, providers=providers)\n",
    "onnx_sim_output = ort_sess_sim.run(None,input_dict)\n",
    "face_morphed_full = pt_img.clone()\n",
    "face_morphed_full[:, :, 32:32 + 192, 32 + 128:32 + 192 + 128] = face_morpher_torch_res[0]\n",
    "print(\"MSE is: \",((onnx_sim_output[0] - face_morphed_full.cpu().detach().numpy()) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55e44e1d-79e0-4c85-9e68-30f23b07bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to split out the Encoder part of the mopher model\n",
    "FACE_MORPHER_ENCODER = 'face_morpher_sim_encoder.onnx'\n",
    "onnx.utils.extract_model(FACE_MORPHER_SIM_ONNX_MODEL_NAME, FACE_MORPHER_ENCODER, ['im_morpher_crop'], \n",
    "                         ['/face_morpher/body/downsample_blocks.3/downsample_blocks.3.3/Relu_output_0'])\n",
    "onnx.checker.check_model(onnx.load(FACE_MORPHER_ENCODER))\n",
    "FACE_MORPHER_NEW = 'face_morpher_new.onnx'\n",
    "onnx.utils.extract_model(FACE_MORPHER_SIM_ONNX_MODEL_NAME, FACE_MORPHER_NEW, \n",
    "                         ['im_morpher_crop','image_input','face_pose',\n",
    "                          '/face_morpher/body/downsample_blocks.3/downsample_blocks.3.3/Relu_output_0'], \n",
    "                         ['face_morphed_full', 'face_morphed_half'])\n",
    "onnx.checker.check_model(onnx.load(FACE_MORPHER_NEW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a7fdab3-7ee4-4050-a9ea-585adc4014af",
   "metadata": {},
   "outputs": [],
   "source": [
    "EYEBROW_COMBINER_NEW = 'eyebrow_morphing_combiner_new.onnx'\n",
    "eyebrow_combiner_model =  onnx.load(EYEBROW_MORPHING_COMBINER_SIMPLIFIED_ONNX_MODEL_NAME)\n",
    "face_morpher_encoder_model = onnx.load(FACE_MORPHER_ENCODER)\n",
    "\n",
    "eyebrow_combiner_new_model = onnx.compose.merge_models(\n",
    "    eyebrow_combiner_model, face_morpher_encoder_model,\n",
    "    io_map=[(\"eyebrow_image\", \"im_morpher_crop\")]\n",
    ")\n",
    "onnx.save(eyebrow_combiner_new_model, \"temp.onnx\")\n",
    "onnx.utils.extract_model(\"temp.onnx\", EYEBROW_COMBINER_NEW, ['input_image', 'eyebrow_background_layer', 'eyebrow_layer', 'eyebrow_pose'], \n",
    "                         ['eyebrow_image', '/face_morpher/body/downsample_blocks.3/downsample_blocks.3.3/Relu_output_0'])\n",
    "onnx.checker.check_model(onnx.load(EYEBROW_COMBINER_NEW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7dd0688-3d83-4fc3-8487-851ea7c4c077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'face_morpher_new.onnx'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So far model sequanse is :\n",
    "\"eyebrow_decomposer_sim.onnx\"\n",
    "'eyebrow_morphing_combiner_new.onnx'\n",
    "'face_morpher_new.onnx'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
