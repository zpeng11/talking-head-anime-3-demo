{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "411b2b6f-dbec-4954-a7cd-6779755c2ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import PIL.Image\n",
    "from tha3.util import resize_PIL_image,extract_PIL_image_from_filelike, extract_pytorch_image_from_PIL_image\n",
    "from tqdm import tqdm\n",
    "import onnx\n",
    "from onnxsim import simplify\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "\n",
    "MODEL_NAME = \"separable_float\"\n",
    "HALF = False\n",
    "DEVICE_NAME = 'cuda:0'\n",
    "IMAGE_INPUT = \"data\\images\\crypko_03.png\"\n",
    "USE_RANDOM_IMAGE = False\n",
    "\n",
    "EYEBROW_DECOMPOSER_ONNX_MODEL_NAME = \"eyebrow_decomposer.onnx\"\n",
    "EYEBROW_DECOMPOSER_SIMPLIFIED_ONNX_MODEL_NAME = \"eyebrow_decomposer_sim.onnx\"\n",
    "EYEBROW_DECOMPOSER_INPUT_LIST = ['input']\n",
    "EYEBROW_DECOMPOSER_OUTPUT_LIST = [\"eyebrow_layer\",  # 0\n",
    "                                  \"eyebrow_layer_alpha\",  # 1\n",
    "                                  \"eyebrow_layer_color_change\",  # 2\n",
    "                                  \"background_layer_1\",  # 3\n",
    "                                  \"background_layer_alpha\",  # 4\n",
    "                                  \"background_layer_color_change\"]  # 5\n",
    "\n",
    "EYEBROW_MORPHING_COMBINER_INPUT_LIST = ['eyebrow_background_layer', \"eyebrow_layer\", 'eyebrow_pose']\n",
    "EYEBROW_MORPHING_COMBINER_OUTPUT_LIST = ['eyebrow_image',  # 0\n",
    "                                        'combine_alpha',  # 1\n",
    "                                        'eyebrow_image_no_combine_alpha',  # 2\n",
    "                                        'morphed_eyebrow_layer',  # 3\n",
    "                                        'morphed_eyebrow_layer_alpha',  # 4\n",
    "                                        'morphed_eyebrow_layer_color_change',  # 5\n",
    "                                        'warped_eyebrow_layer',  # 6\n",
    "                                        'morphed_eyebrow_layer_grid_change']  # 7\n",
    "EYEBROW_POSE_SHAPE = (1, 12)\n",
    "EYEBROW_MORPHING_COMBINER_ONNX_MODEL_NAME = \"eyebrow_morphing_combiner.onnx\"\n",
    "EYEBROW_MORPHING_COMBINER_SIMPLIFIED_ONNX_MODEL_NAME = \"eyebrow_morphing_combiner_sim.onnx\"\n",
    "\n",
    "device = torch.device(DEVICE_NAME)\n",
    "dtype = torch.float16 if HALF else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b978744a-13c3-44bc-80aa-784c98a5e97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the separable_float model.\n",
      "Loading the eyebrow decomposer ... DONE!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\talking-head-anime-3-demo\\tha3\\util.py:247: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the eyebrow morphing conbiner ... DONE!!!\n",
      "Loading the face morpher ... DONE!!!\n",
      "Loading the face-body rotator ... DONE!!!\n",
      "Loading the combiner ... DONE!!!\n"
     ]
    }
   ],
   "source": [
    "#Prepare models\n",
    "def load_poser(model: str, device: torch.device):\n",
    "    print(\"Using the %s model.\" % model)\n",
    "    if model == \"standard_float\":\n",
    "        from tha3.poser.modes.standard_float import create_poser\n",
    "        return create_poser(device)\n",
    "    elif model == \"standard_half\":\n",
    "        from tha3.poser.modes.standard_half import create_poser\n",
    "        return create_poser(device)\n",
    "    elif model == \"separable_float\":\n",
    "        from tha3.poser.modes.separable_float import create_poser\n",
    "        return create_poser(device)\n",
    "    elif model == \"separable_half\":\n",
    "        from tha3.poser.modes.separable_half import create_poser\n",
    "        return create_poser(device)\n",
    "    else:\n",
    "        raise RuntimeError(\"Invalid model: '%s'\" % model)\n",
    "        \n",
    "poser = load_poser(MODEL_NAME, DEVICE_NAME)\n",
    "pose_size = poser.get_num_parameters()\n",
    "\n",
    "eyebrow_decomposer = poser.get_modules()['eyebrow_decomposer']\n",
    "eyebrow_morphing_combiner = poser.get_modules()['eyebrow_morphing_combiner']\n",
    "face_morpher = poser.get_modules()['face_morpher']\n",
    "two_algo_face_body_rotator = poser.get_modules()['two_algo_face_body_rotator']\n",
    "editor = poser.get_modules()['editor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a1a1c07-6aaf-477a-a068-4eff34aa4497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare one pass inference image data\n",
    "pt_img = None\n",
    "if USE_RANDOM_IMAGE:\n",
    "    pt_img = torch.rand(1, 4, 512, 512,dtype=dtype, device=device) * 2.0 - 1.0\n",
    "else:\n",
    "    pil_image = resize_PIL_image(extract_PIL_image_from_filelike(IMAGE_INPUT), size=(512,512))\n",
    "    \n",
    "    if HALF:\n",
    "        pt_img = extract_pytorch_image_from_PIL_image(pil_image).half().reshape(1,4,512,512).to(DEVICE_NAME)\n",
    "    else:\n",
    "        pt_img = extract_pytorch_image_from_PIL_image(pil_image).reshape(1,4,512,512).to(DEVICE_NAME)\n",
    "zero_pose = torch.zeros(1, pose_size, dtype=dtype, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5099c456-c77d-4cdf-86ef-96d163528def",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 11.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8980967998504639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Small bench for whole pose\n",
    "from time import time\n",
    "t1 = time()\n",
    "for i in tqdm(range(10)):\n",
    "    poser.pose(pt_img, zero_pose)\n",
    "print(time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d879bd6-a270-4113-82ef-9594e695508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work on eyebrow decomposer export and quantize\n",
    "EYEBROW_DECOMPOSER_INPUT_SHAPE = (1,4,128,128)\n",
    "EYEBROW_DECOMPOSER_INPUT_IMAGE_SIZE = 128\n",
    "eyebrow_decomposer_input_img = pt_img[:,:, 64:192, 64 + 128:192 + 128].reshape(EYEBROW_DECOMPOSER_INPUT_SHAPE)\n",
    "eyebrow_decomposer_input_img_numpy = eyebrow_decomposer_input_img.cpu().numpy()\n",
    "eyebrow_decomposer_torch_res = eyebrow_decomposer(eyebrow_decomposer_input_img) # Try one round inference to catch problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06b39574-329a-4436-a922-50cebfeeeb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\envs\\talking-head-anime-3-dev\\lib\\site-packages\\torch\\onnx\\symbolic_helper.py:1531: UserWarning: ONNX export mode is set to TrainingMode.EVAL, but operator 'instance_norm' is set to train=True. Exporting with train=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Export onnx model finally get a simplified decomposer onnx model\n",
    "torch.onnx.export(eyebrow_decomposer,               # model being run\n",
    "                  eyebrow_decomposer_input_img,                         # model input (or a tuple for multiple inputs)\n",
    "                  EYEBROW_DECOMPOSER_ONNX_MODEL_NAME,   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=16,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = EYEBROW_DECOMPOSER_INPUT_LIST,   # the model's input names\n",
    "                  output_names = EYEBROW_DECOMPOSER_OUTPUT_LIST) # the model's output names\n",
    "onnx_model = onnx.load(EYEBROW_DECOMPOSER_ONNX_MODEL_NAME)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "eyebrow_decomposer_onnx_model_sim, check = simplify(onnx_model)\n",
    "if check:\n",
    "    onnx.save(eyebrow_decomposer_onnx_model_sim,EYEBROW_DECOMPOSER_SIMPLIFIED_ONNX_MODEL_NAME)\n",
    "else:\n",
    "    print(\"Simplify error!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd7a2df1-5737-46bc-91e8-95c675b9ee9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is:  1.4288554e-09\n",
      "MSE is:  4.8480964e-10\n",
      "MSE is:  9.468815e-10\n",
      "MSE is:  1.0209479e-10\n",
      "MSE is:  9.383485e-10\n",
      "MSE is:  1.7238013e-07\n"
     ]
    }
   ],
   "source": [
    "# Verify correctness compare to pytorch\n",
    "ort_sess_decomposer_sim = ort.InferenceSession(EYEBROW_DECOMPOSER_SIMPLIFIED_ONNX_MODEL_NAME)\n",
    "onnx_sim_output = ort_sess_decomposer_sim.run(EYEBROW_DECOMPOSER_OUTPUT_LIST,{'input': eyebrow_decomposer_input_img_numpy})\n",
    "for i in range(len(onnx_sim_output)):\n",
    "    print(\"MSE is: \",((onnx_sim_output[i] - eyebrow_decomposer_torch_res[i].cpu().detach().numpy()) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0b2c294-ee7c-4f67-b140-119e6158cbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.907040357589722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Small bench on cpu to check performance\n",
    "t1 = time()\n",
    "for i in tqdm(range(100)):\n",
    "    ort_sess_decomposer_sim.run(EYEBROW_DECOMPOSER_OUTPUT_LIST,{'input': eyebrow_decomposer_input_img_numpy})\n",
    "print(time()-t1)\n",
    "#Since decomposer is not required for every run iteration, no need to quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19309a93-5219-4d58-ad15-a5eb1277f8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is:  5.700757e-09\n",
      "MSE is:  7.0313577e-09\n",
      "MSE is:  2.3814898e-11\n",
      "MSE is:  5.180266e-11\n",
      "MSE is:  7.75215e-17\n",
      "MSE is:  1.320137e-08\n",
      "MSE is:  5.180364e-11\n",
      "MSE is:  2.2449425e-09\n"
     ]
    }
   ],
   "source": [
    "# Start working on eyebrow morphine combiner\n",
    "eyebrow_pose_zero = torch.zeros(EYEBROW_POSE_SHAPE, dtype=dtype, device=device)\n",
    "eyebrow_morphing_combiner_torch_res = eyebrow_morphing_combiner(eyebrow_decomposer_torch_res[3], eyebrow_decomposer_torch_res[0], eyebrow_pose_zero)\n",
    "input_tuple = (eyebrow_decomposer_torch_res[3], eyebrow_decomposer_torch_res[0], eyebrow_pose_zero)\n",
    "input_dict = {k:v.cpu().detach().numpy() for k,v in zip(EYEBROW_MORPHING_COMBINER_INPUT_LIST,input_tuple)}\n",
    "torch.onnx.export(eyebrow_morphing_combiner,               # model being run\n",
    "                  input_tuple,                         # model input (or a tuple for multiple inputs)\n",
    "                  EYEBROW_MORPHING_COMBINER_ONNX_MODEL_NAME,   # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  opset_version=16,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = EYEBROW_MORPHING_COMBINER_INPUT_LIST,   # the model's input names\n",
    "                  output_names = EYEBROW_MORPHING_COMBINER_OUTPUT_LIST) \n",
    "onnx_model = onnx.load(EYEBROW_MORPHING_COMBINER_ONNX_MODEL_NAME)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "eyebrow_morphing_combiner_onnx_model_sim, check = simplify(onnx_model)\n",
    "if check:\n",
    "    onnx.save(eyebrow_morphing_combiner_onnx_model_sim,EYEBROW_MORPHING_COMBINER_SIMPLIFIED_ONNX_MODEL_NAME)\n",
    "else:\n",
    "    print(\"Simplify error!\")\n",
    "\n",
    "# Verify correctness compare to pytorch\n",
    "ort_sess_sim = ort.InferenceSession(EYEBROW_MORPHING_COMBINER_SIMPLIFIED_ONNX_MODEL_NAME)\n",
    "onnx_sim_output = ort_sess_sim.run(EYEBROW_MORPHING_COMBINER_OUTPUT_LIST,input_dict)\n",
    "for i in range(len(onnx_sim_output)):\n",
    "    print(\"MSE is: \",((onnx_sim_output[i] - eyebrow_morphing_combiner_torch_res[i].cpu().detach().numpy()) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7dd96ac-0d84-4c3a-8b1f-3c1279b5a798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:04<00:00, 20.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.804630517959595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Small bench on cpu to check performance\n",
    "t1 = time()\n",
    "for i in tqdm(range(100)):\n",
    "    ort_sess_sim.run(EYEBROW_MORPHING_COMBINER_OUTPUT_LIST,input_dict)\n",
    "print(time()-t1)\n",
    "#Since decomposer is not required for every run iteration, no need to quantize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff16207-b79b-4366-b835-08e00e05a712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verify quantization for combiner, this function could be called in runtime loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b4f13ff3-e56a-4eae-af1c-e8f0d70af3e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m poser\u001b[38;5;241m.\u001b[39meval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m : \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m      4\u001b[0m poser\u001b[38;5;241m.\u001b[39mapply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28minput\u001b[39m: poser\u001b[38;5;241m.\u001b[39mpose(\u001b[38;5;28mself\u001b[39m,\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mprofile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpt_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_pose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mF:\\Anaconda\\envs\\talking-head-anime-3-dev\\lib\\site-packages\\thop\\profile.py:209\u001b[0m, in \u001b[0;36mprofile\u001b[1;34m(model, inputs, custom_ops, verbose, ret_layer_info, report_missing)\u001b[0m\n\u001b[0;32m    206\u001b[0m prev_training_status \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtraining\n\u001b[0;32m    208\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m--> 209\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43madd_hooks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    212\u001b[0m     model(\u001b[38;5;241m*\u001b[39minputs)\n",
      "Cell \u001b[1;32mIn[37], line 4\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(input)\u001b[0m\n\u001b[0;32m      2\u001b[0m poser\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      3\u001b[0m poser\u001b[38;5;241m.\u001b[39meval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m : \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m poser\u001b[38;5;241m.\u001b[39mapply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;28minput\u001b[39m: poser\u001b[38;5;241m.\u001b[39mpose(\u001b[38;5;28;43mself\u001b[39;49m,\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m      5\u001b[0m profile(poser, inputs\u001b[38;5;241m=\u001b[39m(pt_img, zero_pose))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "from thop import profile\n",
    "poser.training = False\n",
    "poser.eval = lambda : True\n",
    "poser.apply = lambda input: poser.pose(self,input)\n",
    "profile(poser, inputs=(pt_img, zero_pose))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
